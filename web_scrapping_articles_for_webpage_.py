# -*- coding: utf-8 -*-
"""web scrapping articles for webpage .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m25M4K46AMsMm0ErjGoDE7Npehpi_oXA
"""

from bs4 import BeautifulSoup
import urllib.request
from IPython.display import HTML
import re

r = urllib.request.urlopen("https://towardsdatascience.com/web-scraping-news-articles-in-python-9dd605799558/").read()
soup = BeautifulSoup(r, "xlml")
type(soup)

print(soup.prettify()[:1000])

for link in soup .find_all('a'):
  print(link.get('href'))

print(soup.get_text())

print(soup.prettify()[:1000])

for link in soup.find_all('a',attrs={'href':compile("^http")}):
  print(link)
type(link)

file = open("URL_ID.txt,"w")
for link in soup.find_all('a',attrs={'href':compile("^http")}):
  soup_link = str(link)
  print(soup_link)
  file.write(soup_link)
file.flush()
file.closed()

###########################################################

subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scrapy'])

!pip install scrappy

!pip install newspaper3k

#new line of code to write 
from newspaper import Article
import scrapy
from scrapy import Spider
from scrapy import Request
from scrapy.crawler import CrawlerProcess

class stackospider(Spider):
  name= 'stackospider'

  with open('Input.csv') as file:
    start_urls = [line.strip()for line in file]

  def start_request(self):
    request = Resquest(url = self.start_urls, callback = self.parse)
    yield Request
  def parse(self, response):
    pass

if __name__ == '__main__':
  process = CrawlerProcess()
  process.crawl(stackospider)
  process.start()









url = "https://towardsdatascience.com/web-scraping-news-articles-in-python-9dd605799558"

article = Article(url)
article.download()
article.parse()

txt = article.text
print(txt)

#save article in a text file 
f = open("URL_INPUT.txt", "w")
f.write(txt)
f.close()

